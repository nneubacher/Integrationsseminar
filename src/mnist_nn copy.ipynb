{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset. Convert dataset in to numpy array. Then shuffle the data to ensure randomness for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255. # feature normalization\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier teilen wir das Dataset in ein Entwicklungssatz und ein Trainingssatz auf. Die Daten werden auch transponiert, um der erwarteten Eingabeform des neuronalen Netzwerks zu entsprechen. (Features als Spalten und Beispiele als Zeilen) Schließlich werden die Pixelwerte durch Teilen durch 255 normalisiert, sodass sie zwischen 0 und 1 liegen. Diese Normalisierung ist für die Stabilität des Trainings und eine schnellere Konvergenz unerlässlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion initialisiert die Gewichte (W1, W2) und Verzerrungen (b1, b2) unseres neuronalen Netzwerks. Die Initialisierung ist wichtig, da sie den Startpunkt unserer Optimierung beeinflusst.\n",
    "W1 Form: 784 Eingabefeatures, 10 Neuronen in der ersten Schicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sehen wir zwei Aktivierungsfunktionen:\n",
    "\n",
    "    - ReLU (Rectified Linear Unit): Eine häufig verwendete Aktivierungsfunktion für versteckte Schichten. Sie ist rechenintensiv und hilft dabei, das Problem verschwindender Gradienten zu bewältigen.\n",
    "    - Softmax: Wird in der Ausgabeschicht für Probleme der Mehrklassenklassifikation verwendet. Sie wandelt rohe Ausgabewerte in Wahrscheinlichkeiten für jede Klasse um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion berechnet die Vorwärtsdurchlauf des Netzwerks. Sie berechnet anhand der aktuellen Gewichte und Verzerrungen Zwischenwerte (Z1, A1, Z2) und die endgültige Ausgabe A2.\n",
    "\n",
    "Z1 = W1 (10, 784) * X (784, m) = (10, m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, m):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Rückwärtspropagation berechnet, wie viel jedes Gewicht und jede Verzerrung zum Fehler beigetragen hat. Die zurückgegebenen Werte (dW1, db1, dW2, db2) stellen Gradienten dar, die die Richtung und Größe der zur Fehlerreduktion erforderlichen Änderungen anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion aktualisiert die Gewichte und Verzerrungen des Netzwerks in Richtung des negativen Gradienten. Dieser iterative Prozess hilft dem Netzwerk, aus seinen Fehlern zu lernen. 'alpha' ist die Lernrate, die die Schrittgröße jeder Aktualisierung bestimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, m_train)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion führt das Training unseres neuronalen Netzwerks durch. Es verwendet den Gradientenabstieg, um die Gewichte und Verzerrungen kontinuierlich zu aktualisieren. Am Ende des Trainings sollte unser Netzwerk besser auf die Daten abgestimmt sein und genauere Vorhersagen treffen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction(0, W1, b1, W2, b2)\n",
    "test_prediction(1, W1, b1, W2, b2)\n",
    "test_prediction(2, W1, b1, W2, b2)\n",
    "test_prediction(3, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "get_accuracy(dev_predictions, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = widgets.Image(value=b'', format='png', width=280, height=280)\n",
    "\n",
    "btn_clear = widgets.Button(description='Clear')\n",
    "btn_check = widgets.Button(description='Check digit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.new('RGB', (280, 280), 'white')\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "last_point = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_mouse_move(x, y):\n",
    "    global last_point\n",
    "    draw.line([last_point, (x, y)], fill='black', width=10)\n",
    "    last_point = (x, y)\n",
    "    update_canvas()\n",
    "\n",
    "def handle_mouse_down(x, y):\n",
    "    global last_point\n",
    "    last_point = (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_canvas():\n",
    "    # Convert PIL image to bytes and set it as the canvas value\n",
    "    canvas.value = img_to_byte_array(img)\n",
    "\n",
    "def img_to_byte_array(img):\n",
    "    \"\"\"Convert PIL image to byte array.\"\"\"\n",
    "    img_byte_array = bytearray()\n",
    "    with BytesIO() as output:\n",
    "        img.save(output, format='PNG')\n",
    "        img_byte_array = output.getvalue()\n",
    "    return img_byte_array\n",
    "\n",
    "def clear_canvas(button):\n",
    "    global draw\n",
    "    draw.rectangle([(0,0), img.size], fill='white')\n",
    "    update_canvas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_digit(button):\n",
    "    # Convert the image to the appropriate size (28x28) for the neural network\n",
    "    small_img = img.resize((28, 28)).convert('L')  # Convert to grayscale\n",
    "    input_array = np.array(small_img).reshape(784, 1) / 255.  # Normalize\n",
    "    # Now you can use the input_array with your neural network for prediction.\n",
    "    # e.g., prediction = your_nn_function(input_array)\n",
    "    # For now, we'll just display the resized image to confirm it works\n",
    "    display(small_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas.on_msg(handle_mouse_move, 'mouse_move')\n",
    "canvas.on_msg(handle_mouse_down, 'mouse_down')\n",
    "btn_clear.on_click(clear_canvas)\n",
    "btn_check.on_click(check_digit)\n",
    "\n",
    "display(widgets.VBox([canvas, widgets.HBox([btn_clear, btn_check])]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
